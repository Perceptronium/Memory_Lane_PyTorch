{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AlexNet (2012).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zoOi2zaruZj"
      },
      "outputs": [],
      "source": [
        "# Importing basic libs\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Module\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# User verification for device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using {device}.')"
      ],
      "metadata": {
        "id": "RY0hOKS2r9pF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNet(Module):\n",
        "  \"\"\"\n",
        "    AlexNet (2012)\n",
        "    3 x 224 x 224 input\n",
        "    Novelties :\n",
        "      ReLU (became a standard in DL)\n",
        "      Local Response Normalization (LRN was later discarded by Simonyan et al.)\n",
        "      Overlapping pooling\n",
        "      GPU distribution (not implemented here)\n",
        "      Dropout (became a standard in DL for CV)\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super(AlexNet, self).__init__()\n",
        "    # Non-Linearity applied after every layer\n",
        "    self.NL = nn.ReLU()\n",
        "    # Regularization term to limit overfitting\n",
        "    self.REG = nn.Dropout(p = 0.5)\n",
        "    # LRN parameters\n",
        "    self.k = 2\n",
        "    self.n = 5\n",
        "    self.alpha = 1e-4\n",
        "    self.beta = 0.75\n",
        "    # Feature extraction\n",
        "    self.C1 = nn.Conv2d(3, 96, 11, 4) # 96 x 54 x 54 output\n",
        "    self.LRN1 = nn.LocalResponseNorm(self.n, self.alpha, self.beta, self.k) # 96 x 54 x 54 output\n",
        "    self.MAXPOOL1 = nn.MaxPool2d(3, 2) # 96 x 26 x 26 output\n",
        "    self.C2 = nn.Conv2d(96, 256, 5) # 256 x 22 x 22 output\n",
        "    self.LRN2 = nn.LocalResponseNorm(self.n, self.alpha, self.beta, self.k) # 256 x 22 x 22 output\n",
        "    self.MAXPOOL2 = nn.MaxPool2d(3, 2) # 256 x 10 x 10 output\n",
        "    self.C3 = nn.Conv2d(256, 384, 3, 1, 1) # 384 x 10 x 10 output\n",
        "    self.C4 = nn.Conv2d(384, 384, 3, 1, 1) # 384 x 10 x 10 output\n",
        "    self.C5 = nn.Conv2d(384, 256, 3, 1, 1) # 256 x 10 x 10 output\n",
        "    self.MAXPOOL3 = nn.MaxPool2d(3, 2) # 256 x 4 x 4 output\n",
        "    # Classification\n",
        "    self.FC1 = nn.Linear(256 * 4 * 4, 4096) # 4096 x 1 output\n",
        "    self.FC2 = nn.Linear(4096, 4096) # 4096 x 1 output\n",
        "    self.FC3 = nn.Linear(4096, 2) # 10 x 1 output (1000 x 1 in original paper with 1000 classes)\n",
        "    # Normalizing output\n",
        "    self.OUT = nn.Softmax(dim = 1) # 10 x 1 output\n",
        "\n",
        "  def forward(self, X):\n",
        "    y = self.C1(X)\n",
        "    y = self.NL(y)\n",
        "    y = self.LRN1(y)\n",
        "    y = self.MAXPOOL1(y)\n",
        "    y = self.C2(y)\n",
        "    y = self.NL(y)\n",
        "    y = self.LRN2(y)\n",
        "    y = self.MAXPOOL2(y)\n",
        "    y = self.C3(y)\n",
        "    y = self.NL(y)\n",
        "    y = self.C4(y)\n",
        "    y = self.NL(y)\n",
        "    y = self.C5(y)\n",
        "    y = self.NL(y)\n",
        "    y = self.MAXPOOL3(y)\n",
        "    y = torch.reshape(y, (y.shape[0], y.shape[1] * y.shape[2] * y.shape[3]))\n",
        "    y = self.REG(y)\n",
        "    y = self.FC1(y)\n",
        "    y = self.NL(y)\n",
        "    y = self.REG(y)\n",
        "    y = self.FC2(y)\n",
        "    y = self.NL(y)\n",
        "    logits = self.FC3(y)\n",
        "    y = self.NL(y)\n",
        "    probs = self.OUT(logits)\n",
        "    return probs"
      ],
      "metadata": {
        "id": "WrpwzUFjsDjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Just a sanity check\n",
        "X = torch.ones([32, 3, 224, 224])\n",
        "net = AlexNet()\n",
        "y = net(X)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "4aEZmCiXHgXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WARNING : ALEXNET IS A HUGE MODEL AND IS NOT MEANT TO BE TRAINED ON SMALL DATASETS\n",
        "# THIS IS SIMPLY A SAMPLE TRAINING PLATFORM\n",
        "# ALEXNET WILL SEVERELY OVERFIT ON SMALL DATASETS\n",
        "import os\n",
        "%mkdir data\n",
        "%cd /content/data/\n",
        "%ls\n",
        "!wget http://files.fast.ai/data/examples/dogscats.tgz\n",
        "!tar -zxvf dogscats.tgz\n"
      ],
      "metadata": {
        "id": "Lk7PSHmUxLfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/data/dogscats'\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "imagenet_format = transforms.Compose([\n",
        "                transforms.CenterCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                normalize,\n",
        "            ])\n",
        "\n",
        "dsets = {x: datasets.ImageFolder(os.path.join(data_dir, x), imagenet_format)\n",
        "         for x in ['train', 'valid']}\n",
        "\n",
        "os.path.join(data_dir,'train')\n",
        "dset_sizes = {x: len(dsets[x]) for x in ['train', 'valid']}\n",
        "\n",
        "training_loader = DataLoader(dsets['train'], batch_size = 32, shuffle = True)\n",
        "validation_loader = DataLoader(dsets['valid'], batch_size = 5, shuffle = False)"
      ],
      "metadata": {
        "id": "_mcPkD8ExVZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some sanity checks\n",
        "def viz(image):\n",
        "  image = image.numpy().transpose((1, 2, 0))\n",
        "  plt.imshow(image)\n",
        "\n",
        "for i, data in enumerate(training_loader):\n",
        "  print(\"Checking training loader : \")\n",
        "  batch, labels = data\n",
        "  print(f'Batch shape : {batch.shape}')\n",
        "  if i == 3:\n",
        "    break\n",
        "print(\"==================\")\n",
        "for i, data in enumerate(validation_loader):\n",
        "  print(\"Checking validation loader : \")\n",
        "  batch, labels = data\n",
        "  print(f'Batch shape : {batch.shape} batch labels : {labels}')\n",
        "  batch_viz = torchvision.utils.make_grid(batch)\n",
        "  viz(batch_viz)\n",
        "  if i == 3:\n",
        "    break\n",
        "\n",
        "# These are some default training parameters which are tested and indeed lead to learning\n",
        "# but they could be improved\n",
        "alexnet = AlexNet().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 0.001\n",
        "optim = torch.optim.SGD(alexnet.parameters(),lr = lr)\n",
        "\n",
        "# Defining single epoch training pass\n",
        "def trainer(model, dataloader, criterion, optimizer):\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "\n",
        "  for X, y_T in dataloader:\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      X = X.to(device)\n",
        "      y_T = y_T.to(device)\n",
        "\n",
        "      probs = model(X)   \n",
        "      loss = criterion(probs, y_T)\n",
        "      running_loss += loss.item() * X.size(0)\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "  epoch_loss = running_loss / len(dataloader.dataset)\n",
        "  return model, optimizer, epoch_loss\n",
        "\n",
        "# Define single epoch testing pass\n",
        "def tester(model, dataloader, criterion, optimizer):\n",
        "  model.eval()\n",
        "  running_loss = 0.0\n",
        "\n",
        "  for X, y_T in dataloader:\n",
        "      X = X.to(device)\n",
        "      y_T = y_T.to(device)\n",
        "\n",
        "      probs = model(X)\n",
        "      loss = criterion(probs, y_T)\n",
        "      running_loss += loss.item() * X.size(0)\n",
        "  epoch_loss = running_loss / len(dataloader.dataset)\n",
        "  return model, epoch_loss\n",
        "\n",
        "# Custom function to compute accuracy\n",
        "def compute_accuracy(model, dataloader):\n",
        "    correct_preds = 0 \n",
        "    n = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for X, y_T in dataloader:\n",
        "\n",
        "            X = X.to(device)\n",
        "            y_T = y_T.to(device)\n",
        "\n",
        "            probs = model(X)\n",
        "            _, predictions = torch.max(probs, 1)\n",
        "\n",
        "            n += y_T.size(0)\n",
        "            correct_preds += (predictions == y_T).sum()\n",
        "\n",
        "    return correct_preds.float() / n\n",
        "\n",
        "# Defining model trainer\n",
        "def training(model, training_loader, validation_loader, criterion, optimizer, epochs):\n",
        "  train_losses = []\n",
        "  train_acc =[]\n",
        "  valid_losses = []\n",
        "  valid_acc = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    print(f'\\nEpoch {epoch} : ')\n",
        "    print(\"===========\")\n",
        "    \n",
        "    model, optimizer, training_loss = trainer(model, training_loader, criterion, optimizer)\n",
        "    training_acc = compute_accuracy(model, training_loader)\n",
        "    train_losses.append(training_loss)\n",
        "    train_acc.append(training_acc)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      model, valid_loss = tester(model, validation_loader, criterion, optimizer)\n",
        "      validation_acc = compute_accuracy(model, validation_loader)\n",
        "      valid_losses.append(valid_loss)\n",
        "      valid_acc.append(validation_acc)\n",
        "\n",
        "    print(f'Training loss : {training_loss}')\n",
        "    print(f'Training acc : {training_acc}')\n",
        "    print(f'Validation loss : {valid_loss}')\n",
        "    print(f'Validation acc : {validation_acc}')\n",
        "\n",
        "  train_losses = np.array(train_losses) \n",
        "  valid_losses = np.array(valid_losses)\n",
        "  train_acc = np.array(train_acc)\n",
        "  valid_acc = np.array(valid_acc)\n",
        "\n",
        "  fig, ax = plt.subplots(figsize = (10, 5))\n",
        "  ax.plot(train_losses, color='blue', label='Training loss')\n",
        "  ax.plot(train_acc, color='green', label='Training accuracy') \n",
        "  ax.plot(valid_losses, color='red', label='Validation loss')\n",
        "  ax.plot(valid_acc, color='black', label='Validation accuracy')\n",
        "  \n",
        "  ax.set(title=\"Loss evolution\", \n",
        "            xlabel='Epoch',\n",
        "            ylabel='Loss/Accuracy') \n",
        "  ax.legend()\n",
        "  fig.show()\n",
        "\n",
        "training(alexnet, training_loader, validation_loader, criterion, optim, 15)"
      ],
      "metadata": {
        "id": "ZjgqUm6HSYJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dGBMns-IrMav"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}